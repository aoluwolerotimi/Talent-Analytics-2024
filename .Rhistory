times = 1)
trainData <- examiners[trainIndex, ]
testData <- examiners[-trainIndex, ]
# Build the model - logistic regression
model1 <- multinom(separation_indicator ~ race.Asian + race.Black + race.Hispanic + race.other + tenure_days + new_applications + abn_applications + pen_applications + iss_applications + art_unit_hc + Asian_in_art_unit + Black_in_art_unit + White_in_art_unit + Other_in_art_unit, data = trainData)
# Build the model - logistic regression
model1 <- multinom(separation_indicator ~ race.Asian + race.black + race.Hispanic + race.other + tenure_days + new_applications + abn_applications + pen_applications + iss_applications + art_unit_hc + Asian_in_art_unit + Black_in_art_unit + White_in_art_unit + Other_in_art_unit, data = trainData)
# Summary of the model
summary(model1)
# Usual logistic regression methods. No specific consideration to being panel data
# library(caret)
# library(nnet)
# # Convert 'race' to a factor since it's categorical
# examiners$race <- as.factor(examiners$race)
#
# # Creating dummy variables for 'race'
# examiners <- dummyVars("~ race", data = examiners) %>%
#              predict(newdata = examiners) %>%
#              as.data.frame() %>%
#              bind_cols(examiners, .)
# Converting separation_indicator to a factor
examiners$separation_indicator <- as.factor(examiners$separation_indicator)
# Splitting the data into training and testing sets
set.seed(123) # for reproducibility
trainIndex <- createDataPartition(examiners$separation_indicator, p = .8,
list = FALSE,
times = 1)
trainData <- examiners[trainIndex, ]
testData <- examiners[-trainIndex, ]
# Build the model - logistic regression
model1 <- glm(separation_indicator ~ race.Asian + race.black + race.Hispanic + race.other + tenure_days + new_applications + abn_applications + pen_applications + iss_applications + art_unit_hc + Asian_in_art_unit + Black_in_art_unit + White_in_art_unit + Other_in_art_unit, family = binomial, data = trainData)
# Summary of the model
summary(model1)
# Trying for a fixed effects model
model2 <- plm(separation_indicator ~ race.Asian + race.black + race.Hispanic + race.other + tenure_days + new_applications + abn_applications + pen_applications + iss_applications + art_unit_hc + Asian_in_art_unit + Black_in_art_unit + White_in_art_unit + Other_in_art_unit,
data = trainData, model = "within", family = binomial())
# Trying for a fixed effects model
# plm package requires the binary variables to be numeric rather than as factors
trainData$race.Asian <- as.numeric(trainData$race.Asian)
trainData$race.black <- as.numeric(trainData$race.black)
trainData$race.Hispanic <- as.numeric(trainData$race.Hispanic)
trainData$race.other <- as.numeric(trainData$race.other)
trainData$separation_indicator <- as.numeric(trainData$separation_indicator)
model2 <- plm(separation_indicator ~ race.Asian + race.black + race.Hispanic + race.other + tenure_days + new_applications + abn_applications + pen_applications + iss_applications + art_unit_hc + Asian_in_art_unit + Black_in_art_unit + White_in_art_unit + Other_in_art_unit,
data = trainData, model = "within", family = binomial())
# Summary of the model
summary(model2)
vif(model2)
library(car)
vif(model2)
View(cleaned_applications)
# Filter the dataset to keep only the rows where 'quarter' equals 'max_quarter'
reg_dataset <- cleaned_applications %>%
group_by(examiner_id) %>%
filter(quarter == max_quarter) %>%
ungroup()
View(reg_dataset)
# Convert 'race' to a factor since it's categorical
reg_dataset$race <- as.factor(reg_dataset$race)
# Creating dummy variables for 'race'
reg_dataset <- dummyVars("~ race", data = reg_dataset) %>%
predict(newdata = reg_dataset) %>%
as.data.frame() %>%
bind_cols(reg_dataset, .)
# Splitting the data into training and testing sets
set.seed(123) # for reproducibility
trainIndex <- createDataPartition(reg_dataset$separation_indicator, p = .8,
list = FALSE,
times = 1)
trainData <- reg_dataset[trainIndex, ]
testData <- reg_dataset[-trainIndex, ]
# Build the model - logistic regression
model3 <- glm(separation_indicator ~ race.Asian + race.black + race.Hispanic + race.other + tenure_days + new_applications + abn_applications + pen_applications + iss_applications + art_unit_hc + Asian_in_art_unit + Black_in_art_unit + White_in_art_unit + Other_in_art_unit, family = binomial, data = trainData)
# Summary of the model
summary(model3)
reg_dataset$separation_indicator <- as.factor(reg_dataset$separation_indicator)
# Build the model - logistic regression
model3 <- glm(separation_indicator ~ race.Asian + race.black + race.Hispanic + race.other + tenure_days + new_applications + abn_applications + pen_applications + iss_applications + art_unit_hc + Asian_in_art_unit + Black_in_art_unit + White_in_art_unit + Other_in_art_unit, family = binomial, data = trainData)
# Summary of the model
summary(model3)
vif(model3)
trainData$separation_indicator <- as.factor(trainData$separation_indicator)
# Build the model - logistic regression
model3 <- glm(separation_indicator ~ race.Asian + race.black + race.Hispanic + race.other + tenure_days + new_applications + abn_applications + pen_applications + iss_applications + art_unit_hc + Asian_in_art_unit + Black_in_art_unit + White_in_art_unit + Other_in_art_unit, family = binomial, data = trainData)
# Summary of the model
summary(model3)
vif(model3)
# Predict on test data
testData$separation_indicator <- as.factor(testData$separation_indicator)
predictions <- predict(model3, testData)
# Evaluate the model
confusionMatrix(predictions, testData$separation_indicator)
# Build the model - logistic regression
model3 <- glm(separation_indicator ~ race.Asian + race.black + race.Hispanic + race.other + tenure_days + new_applications + abn_applications + pen_applications + iss_applications + art_unit_hc + Asian_in_art_unit + Black_in_art_unit + White_in_art_unit + Other_in_art_unit, family = binomial, data = trainData)
# Summary of the model
summary(model3)
# trying without issued applications variable
# Build the model - logistic regression
model4 <- glm(separation_indicator ~ race.Asian + race.black + race.Hispanic + race.other + tenure_days + new_applications + abn_applications + pen_applications + art_unit_hc + Asian_in_art_unit + Black_in_art_unit + White_in_art_unit + Other_in_art_unit, family = binomial, data = trainData)
# Summary of the model
summary(model4)
# Predict on test data
predictions <- predict(model3, testData)
# Evaluate the model
confusionMatrix(predictions, testData$separation_indicator)
View(testData)
View(trainData)
# Convert predicted probabilities to class labels
# Assuming your model predicts probabilities for class '1'
predicted_probabilities <- predict(model3, testData, type = "response")
predicted_labels <- ifelse(predicted_probabilities > 0.5, 1, 0)
# Convert to factor if not already
predicted_labels <- factor(predicted_labels, levels = c(0, 1))
actual_labels <- factor(testData$separation_indicator, levels = c(0, 1))
# Evaluate the model using confusion matrix
confusionMatrix(predicted_labels, actual_labels)
# get unique values for separation_indicator in reg_dataset
unique(reg_dataset$separation_indicator)
# Convert predicted probabilities to class labels
# Assuming your model predicts probabilities for class '1'
predicted_probabilities <- predict(model3, testData, type = "response")
predicted_labels <- ifelse(predicted_probabilities > 0.5, 1, 0)
# Convert to factor if not already
# Ensure '1' is the first level in the factor
predicted_labels <- factor(predicted_labels, levels = c(1, 0))
actual_labels <- factor(testData$separation_indicator, levels = c(1, 0))
# Evaluate the model using confusion matrix
confusionMatrix(predicted_labels, actual_labels)
library(pROC)
# Create the ROC curve
roc_curve <- roc(testData$separation_indicator, predicted_probabilities)
# Plot the ROC curve
plot(roc_curve, main = "ROC Curve", col = "blue")
abline(a = 0, b = 1, lty = 2, col = "red")  # Adds a diagonal reference line
# Adding AUC (Area Under Curve) to the plot
auc(roc_curve)
knitr::opts_chunk$set(echo = TRUE)
library(arrow)
library(tidyverse)
library(dplyr)
library(lubridate)
library(plm)
data_path <- "/Users/aoluwolerotimi/Datasets/"
applications <- read_feather(paste0(data_path,"app_data_w_indicators.feather"))
View(applications)
# Usual logistic regression methods. No specific consideration to being panel data
library(caret)
library(nnet)
# # Convert 'race' to a factor since it's categorical
# examiners$race <- as.factor(examiners$race)
#
# # Creating dummy variables for 'race'
# examiners <- dummyVars("~ race", data = examiners) %>%
#              predict(newdata = examiners) %>%
#              as.data.frame() %>%
#              bind_cols(examiners, .)
# Converting separation_indicator to a factor
# examiners$separation_indicator <- as.factor(examiners$separation_indicator)
# Filter the dataset to keep only the rows where 'quarter' equals 'max_quarter'
reg_dataset <- cleaned_applications %>%
group_by(examiner_id) %>%
filter(quarter == max_quarter) %>%
ungroup()
cleaned_applications <- applications %>%
select(-au_move) %>% # Drop the au_move column
group_by(examiner_id, quarter) %>%
arrange(desc(separation_indicator)) %>% # Prioritize rows with separation_indicator = 1
filter(if (any(separation_indicator == 1)) {
row_number() == which(separation_indicator == 1)[1] # Keep the first row with separation_indicator = 1
} else {
row_number() == 1 # If no 1s, keep the first row
}) %>%
ungroup()
# Filter the dataset to keep only the rows where 'quarter' equals 'max_quarter'
reg_dataset <- cleaned_applications %>%
group_by(examiner_id) %>%
filter(quarter == max_quarter) %>%
ungroup()
# Convert 'race' to a factor since it's categorical
reg_dataset$race <- as.factor(reg_dataset$race)
# Creating dummy variables for 'race'
reg_dataset <- dummyVars("~ race", data = reg_dataset) %>%
predict(newdata = reg_dataset) %>%
as.data.frame() %>%
bind_cols(reg_dataset, .)
View(reg_dataset)
# Splitting the data into training and testing sets
set.seed(123) # for reproducibility
trainIndex <- createDataPartition(reg_dataset$separation_indicator, p = .8,
list = FALSE,
times = 1)
trainData <- reg_dataset[trainIndex, ]
testData <- reg_dataset[-trainIndex, ]
# Build the model - logistic regression
model3 <- glm(separation_indicator ~ race.Asian + race.black + race.Hispanic + race.other + tenure_days + new_applications + abn_applications + pen_applications + iss_applications + art_unit_hc + Asian_in_art_unit + Black_in_art_unit + White_in_art_unit + Other_in_art_unit, family = binomial, data = trainData)
# Summary of the model
summary(model3)
# Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred. Investigate later
# Convert predicted probabilities to class labels
# Assuming your model predicts probabilities for class '1'
predicted_probabilities <- predict(model3, testData, type = "response")
predicted_labels <- ifelse(predicted_probabilities > 0.5, 1, 0)
# Convert to factor if not already
# Ensure '1' is the first level in the factor
predicted_labels <- factor(predicted_labels, levels = c(1, 0))
actual_labels <- factor(testData$separation_indicator, levels = c(1, 0))
# Evaluate the model using confusion matrix
confusionMatrix(predicted_labels, actual_labels)
library(pROC)
# Create the ROC curve
roc_curve <- roc(testData$separation_indicator, predicted_probabilities)
# Plot the ROC curve
plot(roc_curve, main = "ROC Curve", col = "blue")
abline(a = 0, b = 1, lty = 2, col = "red")  # Adds a diagonal reference line
# Adding AUC (Area Under Curve) to the plot
auc(roc_curve) # area under the curve was 0.8976. Probably overfitted
installed.packages("ROCR")
install.packages("ROCR")
#### Load the lib and data
library(tidyverse)
library(lubridate)
library(arrow)
library(dplyr)
library(ggplot2)
library(pROC)
library(caret)
data_path <- "/Users/aoluwolerotimi/Datasets/"
panel_df <- read_feather(paste0(data_path,"app_data_panel_b.feather"))
# # Converting race into dummy variables
# # Convert 'race' to a factor since it's categorical
# panel_df$race <- as.factor(panel_df$race)
#
# # Creating dummy variables for 'race'
# panel_df <- dummyVars("~ race", data = panel_df) %>%
#              predict(newdata = panel_df) %>%
#              as.data.frame() %>%
#              bind_cols(panel_df, .)
#
# #Splitting the Data
# set.seed(123) # for reproducibility & ensuring representativeness of classes in both sets
# splitIndex <- createDataPartition(panel_df$separation_indicator, p = .70, list = FALSE, times = 1)
# train_data <- panel_df[splitIndex,]
# test_data <- panel_df[-splitIndex,]
#### Load the lib and data
library(tidyverse)
library(lubridate)
library(arrow)
library(dplyr)
library(ggplot2)
library(pROC)
library(caret)
data_path <- "/Users/aoluwolerotimi/Datasets/"
panel_df <- read_feather(paste0(data_path,"app_data_panel_b.feather"))
# # Converting race into dummy variables
# # Convert 'race' to a factor since it's categorical
# panel_df$race <- as.factor(panel_df$race)
#
# # Creating dummy variables for 'race'
# panel_df <- dummyVars("~ race", data = panel_df) %>%
#              predict(newdata = panel_df) %>%
#              as.data.frame() %>%
#              bind_cols(panel_df, .)
#
# #Splitting the Data
# set.seed(123) # for reproducibility & ensuring representativeness of classes in both sets
# splitIndex <- createDataPartition(panel_df$separation_indicator, p = .70, list = FALSE, times = 1)
# train_data <- panel_df[splitIndex,]
# test_data <- panel_df[-splitIndex,]
View(panel_df)
# data_path <- "/Users/aoluwolerotimi/Datasets/"
# panel_df <- read_feather(paste0(data_path,"app_data_panel_b.feather"))
# Converting race into dummy variables
# Convert 'race' to a factor since it's categorical
panel_df$race <- as.factor(panel_df$race)
# Creating dummy variables for 'race'
panel_df <- dummyVars("~ race", data = panel_df) %>%
predict(newdata = panel_df) %>%
as.data.frame() %>%
bind_cols(panel_df, .)
#
# #Splitting the Data
# set.seed(123) # for reproducibility & ensuring representativeness of classes in both sets
# splitIndex <- createDataPartition(panel_df$separation_indicator, p = .70, list = FALSE, times = 1)
# train_data <- panel_df[splitIndex,]
# test_data <- panel_df[-splitIndex,]
# data_path <- "/Users/aoluwolerotimi/Datasets/"
# panel_df <- read_feather(paste0(data_path,"app_data_panel_b.feather"))
# Converting race into dummy variables
# Convert 'race' to a factor since it's categorical
# panel_df$race <- as.factor(panel_df$race)
#
# # Creating dummy variables for 'race'
# panel_df <- dummyVars("~ race", data = panel_df) %>%
#              predict(newdata = panel_df) %>%
#              as.data.frame() %>%
#              bind_cols(panel_df, .)
# #
#Splitting the Data
set.seed(123) # for reproducibility & ensuring representativeness of classes in both sets
splitIndex <- createDataPartition(panel_df$separation_indicator, p = .70, list = FALSE, times = 1)
train_data <- panel_df[splitIndex,]
test_data <- panel_df[-splitIndex,]
#Building the Logistic Regression Model
model <- glm(separation_indicator ~ new_applications + ISSUED_applications + total_abn_applications +
total_PEN_applications + tenure_days +  Asian_in_art_unit +
Black_in_art_unit + Other_in_art_unit + White_in_art_unit +
au_move_indicator + race.Asian + race.black + race.Hispanic + race.other,
data = train_data, family = "binomial")
#Building the Logistic Regression Model
model <- glm(separation_indicator ~ new_applications + ISSUED_applications+
total_PEN_applications + tenure_days +  Asian_in_art_unit +
Black_in_art_unit + Other_in_art_unit + White_in_art_unit +
au_move_indicator + race.Asian + race.black + race.Hispanic + race.other,
data = train_data, family = "binomial")
data_path <- "/Users/aoluwolerotimi/Datasets/"
panel_df <- read_feather(paste0(data_path,"app_panel_df.feather"))
# Converting race into dummy variables
# Convert 'race' to a factor since it's categorical
# panel_df$race <- as.factor(panel_df$race)
#
# # Creating dummy variables for 'race'
# panel_df <- dummyVars("~ race", data = panel_df) %>%
#              predict(newdata = panel_df) %>%
#              as.data.frame() %>%
#              bind_cols(panel_df, .)
#
# #Splitting the Data
# set.seed(123) # for reproducibility & ensuring representativeness of classes in both sets
# splitIndex <- createDataPartition(panel_df$separation_indicator, p = .70, list = FALSE, times = 1)
# train_data <- panel_df[splitIndex,]
# test_data <- panel_df[-splitIndex,]
data_path <- "/Users/aoluwolerotimi/Datasets/"
panel_df <- read_feather(paste0(data_path,"app_panel_df.feather"))
# Converting race into dummy variables
# Convert 'race' to a factor since it's categorical
panel_df$race <- as.factor(panel_df$race)
# Creating dummy variables for 'race'
panel_df <- dummyVars("~ race", data = panel_df) %>%
predict(newdata = panel_df) %>%
as.data.frame() %>%
bind_cols(panel_df, .)
#Splitting the Data
set.seed(123) # for reproducibility & ensuring representativeness of classes in both sets
splitIndex <- createDataPartition(panel_df$separation_indicator, p = .70, list = FALSE, times = 1)
train_data <- panel_df[splitIndex,]
test_data <- panel_df[-splitIndex,]
View(panel_df)
#Building the Logistic Regression Model
model <- glm(separation_indicator ~ new_applications + ISSUED_applications + total_abn_applications +
total_PEN_applications + tenure_days +  Asian_in_art_unit +
Black_in_art_unit + Other_in_art_unit + White_in_art_unit +
au_move_indicator + race.Asian + race.black + race.Hispanic + race.other,
data = train_data, family = "binomial")
# gender and women in art unit excluded due to missing data (previously imputed 0 for N/A in Women in Art unit but realized that wasn't a good approach since the reason it was N/A was because the associated gender data for women was missing )
summary(model)
#Evaluating the Model & Creating Confusion Matrix
# Convert predicted probabilities to class labels
# Assuming your model predicts probabilities for class '1'
predicted_probabilities <- predict(model, test_data, type = "response")
predicted_labels <- ifelse(predicted_probabilities > 0.5, 1, 0)
# Convert to factor if not already
# Ensure '1' is the first level in the factor
predicted_labels <- factor(predicted_labels, levels = c(1, 0))
actual_labels <- factor(test_data$separation_indicator, levels = c(1, 0))
# Evaluate the model using confusion matrix
confusionMatrix(predicted_labels, actual_labels)
# ORIGINAL CODE SNIPPET
# # Predicting on the test set
# predictions <- predict(model, test_data, type = "response")
#
# # Binarize predictions based on a 0.5 cutoff
# binarized_predictions <- ifelse(predictions > 0.5, 1, 0)
# Generating the ROC curve data
roc_data <- roc(test_data$separation_indicator, binarized_predictions)
# Generating the ROC curve data
roc_data <- roc(test_data$separation_indicator, predicted_labels)
# Load the ROCR library
library(ROCR)
# Predicting on the test set with probabilities
panel_df_pHat = predict(model, test_data, type = 'response')
# Check for NAs in predictions and handle them
if (any(is.na(panel_df_pHat))) {
# Handle NAs - Exclude NAs
valid_indices <- which(!is.na(panel_df_pHat))
panel_df_pHat <- panel_df_pHat[valid_indices]
actual_outcomes <- test_data$separation_indicator[valid_indices]
} else {
actual_outcomes <- test_data$separation_indicator
}
# Ensure that predictions and actual outcomes are in the correct format
panel_df_pHat <- as.numeric(panel_df_pHat)
actual_outcomes <- as.numeric(actual_outcomes)
# Creating a prediction object with actual outcomes and predicted probabilities
panel_df_prediction = prediction(panel_df_pHat, actual_outcomes)
# Evaluating performance for tpr and fpr
panel_df_performance = performance(panel_df_prediction, "tpr", "fpr")
# Plotting the ROC Curve
plot(panel_df_performance, main = "ROC Curve", colorize = TRUE)
# Load the ROCR library
#library(ROCR)
library(pROC)
# Calculating the AUC
panel_df_auc = performance(panel_df_prediction, measure = "auc")
# Extract the AUC value
auc_value = panel_df_auc@y.values[[1]]
# Printing the AUC
print(paste("AUC:", auc_value))
panel_df %>%
summarise_all(~ sum(is.na(.)))
# Ordering the data frame by examiner_id, quarter (descending), and separation_indicator
# This ensures that for each examiner_id, the highest quarter comes first
# and within the same quarter, records with separation_indicator = 1 come first in case of accidental duplicates
ordered_panel_df <- panel_df %>%
arrange(examiner_id, desc(quarter), desc(separation_indicator))
# Selecting the first record for each examiner_id
collapsed_df <- ordered_panel_df %>%
group_by(examiner_id) %>%
slice(1) %>%
ungroup()
table(collapsed_df$separation_indicator)
View(collapsed_df)
#Splitting the Data
set.seed(123) # for reproducibility & ensuring representativeness of classes in both sets
splitIndex <- createDataPartition(collapsed_df$separation_indicator, p = .70, list = FALSE, times = 1)
train_data2 <- collapsed_df[splitIndex,]
test_data2 <- collapsed_df[-splitIndex,]
model2 <- glm(separation_indicator ~ new_applications + ISSUED_applications + total_abn_applications +
total_PEN_applications + tenure_days +  Asian_in_art_unit +
Black_in_art_unit + Other_in_art_unit + White_in_art_unit +
au_move_indicator + race.Asian + race.black + race.Hispanic + race.other,
data = train_data2, family = "binomial")
summary(model2)
model2 <- glm(separation_indicator ~ new_applications + ISSUED_applications + total_abn_applications +
total_PEN_applications + tenure_days +  Asian_in_art_unit +
Black_in_art_unit + Other_in_art_unit + White_in_art_unit +
au_move_indicator + race.Asian + race.black + race.Hispanic + race.other,
data = train_data2, family = "binomial")
summary(model2)
#Evaluating the Model & Creating Confusion Matrix
# Convert predicted probabilities to class labels
# Assuming your model predicts probabilities for class '1'
predicted_probabilities2 <- predict(model2, test_data2, type = "response")
predicted_labels2 <- ifelse(predicted_probabilities2 > 0.5, 1, 0)
# Convert to factor if not already
# Ensure '1' is the first level in the factor
predicted_labels2 <- factor(predicted_labels2, levels = c(1, 0))
actual_labels2 <- factor(test_data2$separation_indicator, levels = c(1, 0))
# Evaluate the model using confusion matrix
confusionMatrix(predicted_labels2, actual_labels2)
#Evaluating the Model & Creating Confusion Matrix
# Convert predicted probabilities to class labels
# Assuming your model predicts probabilities for class '1'
predicted_probabilities2 <- predict(model2, test_data2, type = "response")
predicted_labels2 <- ifelse(predicted_probabilities2 > 0.5, 1, 0)
# Convert to factor if not already
# Ensure '1' is the first level in the factor
predicted_labels2 <- factor(predicted_labels2, levels = c(1, 0))
actual_labels2 <- factor(test_data2$separation_indicator, levels = c(1, 0))
# Evaluate the model using confusion matrix
confusionMatrix(predicted_labels2, actual_labels2)
# Predicting on the test set with probabilities
collapsed_df_pHat = predict(model2, test_data2, type = 'response')
# Check for NAs in predictions and handle them
if (any(is.na(collapsed_df_pHat))) {
# Handle NAs - Exclude NAs
valid_indices <- which(!is.na(collapsed_df_pHat))
collapsed_df_pHat <- collapsed_df_pHat[valid_indices]
actual_outcomes <- test_data2$separation_indicator[valid_indices]
} else {
actual_outcomes <- test_data2$separation_indicator
}
# Ensure that predictions and actual outcomes are in the correct format
collapsed_df_pHat <- as.numeric(collapsed_df_pHat)
actual_outcomes <- as.numeric(actual_outcomes)
# Creating a prediction object with actual outcomes and predicted probabilities
collapsed_dfprediction = prediction(collapsed_df_pHat, actual_outcomes)
# Evaluating performance for tpr and fpr
collapsed_dfperformance = performance(collapsed_df_prediction, "tpr", "fpr")
# Predicting on the test set with probabilities
collapsed_df_pHat = predict(model2, test_data2, type = 'response')
# Check for NAs in predictions and handle them
if (any(is.na(collapsed_df_pHat))) {
# Handle NAs - Exclude NAs
valid_indices <- which(!is.na(collapsed_df_pHat))
collapsed_df_pHat <- collapsed_df_pHat[valid_indices]
actual_outcomes <- test_data2$separation_indicator[valid_indices]
} else {
actual_outcomes <- test_data2$separation_indicator
}
# Ensure that predictions and actual outcomes are in the correct format
collapsed_df_pHat <- as.numeric(collapsed_df_pHat)
actual_outcomes <- as.numeric(actual_outcomes)
# Creating a prediction object with actual outcomes and predicted probabilities
collapsed_df_prediction = prediction(collapsed_df_pHat, actual_outcomes)
# Evaluating performance for tpr and fpr
collapsed_df_performance = performance(collapsed_df_prediction, "tpr", "fpr")
# Plotting the ROC Curve
plot(collapsed_df_performance, main = "ROC Curve", colorize = TRUE)
# Calculating the AUC
collapsed_df_auc = performance(collapsed_df_prediction, measure = "auc")
# Extract the AUC value
auc_value = collapsed_df_auc@y.values[[1]]
# Printing the AUC
print(paste("AUC:", auc_value))
library(psych)
summary_stats_numeric <- describe(collapsed_df)
summary_stats_categorical <- describeBy(
collapsed_df,
group = collapsed_df$factor_variable
)
summary_stats_numeric
summary_stats_categorical
