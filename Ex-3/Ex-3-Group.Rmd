---
title: "E3"
output: pdf_document
date: "2024-01-29"
---

```{r}
#### Load the lib and data
library(tidyverse)
library(lubridate)
library(arrow)
library(dplyr)
library(ggplot2)
library(pROC)
library(caret)
```

### Prediction of seperation using full dataset - not yet providing consideration for being panel data

```{r}

data_path <- "/Users/aoluwolerotimi/Datasets/"
panel_df <- read_feather(paste0(data_path,"app_panel_df.feather"))


# Converting race into dummy variables
# Convert 'race' to a factor since it's categorical
panel_df$race <- as.factor(panel_df$race)

# Creating dummy variables for 'race'
panel_df <- dummyVars("~ race", data = panel_df) %>%
             predict(newdata = panel_df) %>%
             as.data.frame() %>%
             bind_cols(panel_df, .)

#Splitting the Data
set.seed(123) # for reproducibility & ensuring representativeness of classes in both sets
splitIndex <- createDataPartition(panel_df$separation_indicator, p = .70, list = FALSE, times = 1)
train_data <- panel_df[splitIndex,]
test_data <- panel_df[-splitIndex,]
```

```{r}
#Building the Logistic Regression Model

model <- glm(separation_indicator ~ new_applications + ISSUED_applications + total_abn_applications +
             total_PEN_applications + tenure_days +  Asian_in_art_unit +
             Black_in_art_unit + Other_in_art_unit + White_in_art_unit +
             au_move_indicator + race.Asian + race.black + race.Hispanic + race.other,
             data = train_data, family = "binomial")

# gender and women in art unit excluded due to missing data (previously imputed 0 for N/A in Women in Art unit but realized that wasn't a good approach since the reason it was N/A was because the associated gender data for women was missing )

summary(model)
```

```{r}
#Evaluating the Model & Creating Confusion Matrix

# Convert predicted probabilities to class labels
# Assuming your model predicts probabilities for class '1'
predicted_probabilities <- predict(model, test_data, type = "response")
predicted_labels <- ifelse(predicted_probabilities > 0.5, 1, 0)


# Convert to factor if not already
# Ensure '1' is the first level in the factor
predicted_labels <- factor(predicted_labels, levels = c(1, 0))
actual_labels <- factor(test_data$separation_indicator, levels = c(1, 0))

# Evaluate the model using confusion matrix
confusionMatrix(predicted_labels, actual_labels)
```

```{r}
# Load the ROCR library
library(ROCR)
```

```{r}
# Predicting on the test set with probabilities
panel_df_pHat = predict(model, test_data, type = 'response')

# Check for NAs in predictions and handle them
if (any(is.na(panel_df_pHat))) {
  # Handle NAs - Exclude NAs
  valid_indices <- which(!is.na(panel_df_pHat))
  panel_df_pHat <- panel_df_pHat[valid_indices]
  actual_outcomes <- test_data$separation_indicator[valid_indices]
} else {
  actual_outcomes <- test_data$separation_indicator
}

# Ensure that predictions and actual outcomes are in the correct format
panel_df_pHat <- as.numeric(panel_df_pHat)
actual_outcomes <- as.numeric(actual_outcomes)

# Creating a prediction object with actual outcomes and predicted probabilities
panel_df_prediction = prediction(panel_df_pHat, actual_outcomes)

# Evaluating performance for tpr and fpr
panel_df_performance = performance(panel_df_prediction, "tpr", "fpr")

# Plotting the ROC Curve
plot(panel_df_performance, main = "ROC Curve", colorize = TRUE)

```

```{r}
# Calculating the AUC
panel_df_auc = performance(panel_df_prediction, measure = "auc")

# Extract the AUC value
auc_value = panel_df_auc@y.values[[1]]

# Printing the AUC
print(paste("AUC:", auc_value))
```

### Prediction of seperation using collapsed dataset, one entry per examiner

```{r}
# Ordering the data frame by examiner_id, quarter (descending), and separation_indicator
# This ensures that for each examiner_id, the highest quarter comes first
# and within the same quarter, records with separation_indicator = 1 come first in case of accidental duplicates
ordered_panel_df <- panel_df %>%
  arrange(examiner_id, desc(quarter), desc(separation_indicator))

# Selecting the first record for each examiner_id
collapsed_df <- ordered_panel_df %>%
  group_by(examiner_id) %>%
  slice(1) %>%
  ungroup()
```

```{r}
#Splitting the Data
set.seed(123) # for reproducibility & ensuring representativeness of classes in both sets
splitIndex <- createDataPartition(collapsed_df$separation_indicator, p = .70, list = FALSE, times = 1)
train_data2 <- collapsed_df[splitIndex,]
test_data2 <- collapsed_df[-splitIndex,]
```

```{r}

model2 <- glm(separation_indicator ~ new_applications + ISSUED_applications + total_abn_applications +
             total_PEN_applications + tenure_days +  Asian_in_art_unit +
             Black_in_art_unit + Other_in_art_unit + White_in_art_unit +
             au_move_indicator + race.Asian + race.black + race.Hispanic + race.other,
             data = train_data2, family = "binomial")

summary(model2)
```


```{r}
#Evaluating the Model & Creating Confusion Matrix

# Convert predicted probabilities to class labels
# Assuming your model predicts probabilities for class '1'
predicted_probabilities2 <- predict(model2, test_data2, type = "response")
predicted_labels2 <- ifelse(predicted_probabilities2 > 0.5, 1, 0)


# Convert to factor if not already
# Ensure '1' is the first level in the factor
predicted_labels2 <- factor(predicted_labels2, levels = c(1, 0))
actual_labels2 <- factor(test_data2$separation_indicator, levels = c(1, 0))

# Evaluate the model using confusion matrix
confusionMatrix(predicted_labels2, actual_labels2)
```

```{r}

# Predicting on the test set with probabilities
collapsed_df_pHat = predict(model2, test_data2, type = 'response')

# Check for NAs in predictions and handle them
if (any(is.na(collapsed_df_pHat))) {
  # Handle NAs - Exclude NAs
  valid_indices <- which(!is.na(collapsed_df_pHat))
  collapsed_df_pHat <- collapsed_df_pHat[valid_indices]
  actual_outcomes <- test_data2$separation_indicator[valid_indices]
} else {
  actual_outcomes <- test_data2$separation_indicator
}

# Ensure that predictions and actual outcomes are in the correct format
collapsed_df_pHat <- as.numeric(collapsed_df_pHat)
actual_outcomes <- as.numeric(actual_outcomes)

# Creating a prediction object with actual outcomes and predicted probabilities
collapsed_df_prediction = prediction(collapsed_df_pHat, actual_outcomes)

# Evaluating performance for tpr and fpr
collapsed_df_performance = performance(collapsed_df_prediction, "tpr", "fpr")

# Plotting the ROC Curve
plot(collapsed_df_performance, main = "ROC Curve", colorize = TRUE)

```

```{r}
# Calculating the AUC
collapsed_df_auc = performance(collapsed_df_prediction, measure = "auc")

# Extract the AUC value
auc_value = collapsed_df_auc@y.values[[1]]

# Printing the AUC
print(paste("AUC:", auc_value))
```

```{r}
library(psych)
summary_stats_numeric <- describe(collapsed_df)
 
summary_stats_categorical <- describeBy(
  collapsed_df,
  group = collapsed_df$factor_variable
)
 
summary_stats_numeric
 
summary_stats_categorical
```